% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{ramakrishna2025lumellmunlearningmultitask,
      title={LUME: LLM Unlearning with Multitask Evaluations}, 
      author={Anil Ramakrishna and Yixin Wan and Xiaomeng Jin and Kai-Wei Chang and Zhiqi Bu and Bhanukiran Vinzamuri and Volkan Cevher and Mingyi Hong and Rahul Gupta},
      journal={arXiv preprint arXiv:2502.15097},
      year={2025},
}

@article{wang2024llm,
  title={LLM Unlearning via Loss Adjustment with Only Forget Data},
  author={Wang, Yaxuan and Wei, Jiaheng and Liu, Chris Yuhao and Pang, Jinlong and Liu, Quan and Shah, Ankit Parag and Bao, Yujia and Liu, Yang and Wei, Wei},
  journal={arXiv preprint arXiv:2410.11143},
  year={2024}
}

@article{veldanda2024llm,
  title={LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models},
  author={Veldanda, Akshaj Kumar and Zhang, Shi-Xiong and Das, Anirban and Chakraborty, Supriyo and Rawls, Stephen and Sahu, Sambit and Naphade, Milind},
  journal={arXiv preprint arXiv:2409.13054},
  year={2024}
}

@article{jang2022knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.01504},
  year={2022}
}

@article{choi2024snap,
  title={Snap: Unlearning selective knowledge in large language models with negative instructions},
  author={Choi, Minseok and Rim, Daniel and Lee, Dohyun and Choo, Jaegul},
  journal={arXiv preprint arXiv:2406.12329},
  year={2024}
}

@inproceedings{shi2024ulmr,
  title={ULMR: Unlearning Large Language Models via Negative Response and Model Parameter Average},
  author={Shi, Shaojie and Tan, Xiaoyu and Qiu, Xihe and Qu, Chao and Nie, Kexin and Cheng, Yuan and Chu, Wei and Yinghui, Xu and Qi, Yuan},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={755--762},
  year={2024}
}

@article{eldan2023s,
  title={Who's harry potter? approximate unlearning in llms},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@article{mekala2024alternate,
  title={Alternate preference optimization for unlearning factual knowledge in large language models},
  author={Mekala, Anmol and Dorna, Vineeth and Dubey, Shreya and Lalwani, Abhishek and Koleczek, David and Rungta, Mukund and Hasan, Sadid and Lobo, Elita},
  journal={arXiv preprint arXiv:2409.13474},
  year={2024}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}
@misc{tofu2024,
      title={TOFU: A Task of Fictitious Unlearning for LLMs}, 
      author={Pratyush Maini and Zhili Feng and Avi Schwarzschild and Zachary C. Lipton and J. Zico Kolter},
      year={2024},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}