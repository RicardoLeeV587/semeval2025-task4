\begin{thebibliography}{9}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Choi et~al.(2024)Choi, Rim, Lee, and Choo}]{choi2024snap}
Minseok Choi, Daniel Rim, Dohyun Lee, and Jaegul Choo. 2024.
\newblock Snap: Unlearning selective knowledge in large language models with negative instructions.
\newblock \emph{arXiv preprint arXiv:2406.12329}.

\bibitem[{Eldan and Russinovich(2023)}]{eldan2023s}
Ronen Eldan and Mark Russinovich. 2023.
\newblock Who's harry potter? approximate unlearning in llms.
\newblock \emph{arXiv preprint arXiv:2310.02238}.

\bibitem[{Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, Chen et~al.}]{hu2022lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, Weizhu Chen, et~al. 2022.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{ICLR}, 1(2):3.

\bibitem[{Jang et~al.(2022)Jang, Yoon, Yang, Cha, Lee, Logeswaran, and Seo}]{jang2022knowledge}
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022.
\newblock Knowledge unlearning for mitigating privacy risks in language models.
\newblock \emph{arXiv preprint arXiv:2210.01504}.

\bibitem[{Mekala et~al.(2024)Mekala, Dorna, Dubey, Lalwani, Koleczek, Rungta, Hasan, and Lobo}]{mekala2024alternate}
Anmol Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid Hasan, and Elita Lobo. 2024.
\newblock Alternate preference optimization for unlearning factual knowledge in large language models.
\newblock \emph{arXiv preprint arXiv:2409.13474}.

\bibitem[{Ramakrishna et~al.(2025)Ramakrishna, Wan, Jin, Chang, Bu, Vinzamuri, Cevher, Hong, and Gupta}]{ramakrishna2025lumellmunlearningmultitask}
Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, and Rahul Gupta. 2025.
\newblock Lume: Llm unlearning with multitask evaluations.
\newblock \emph{arXiv preprint arXiv:2502.15097}.

\bibitem[{Shi et~al.(2024)Shi, Tan, Qiu, Qu, Nie, Cheng, Chu, Yinghui, and Qi}]{shi2024ulmr}
Shaojie Shi, Xiaoyu Tan, Xihe Qiu, Chao Qu, Kexin Nie, Yuan Cheng, Wei Chu, Xu~Yinghui, and Yuan Qi. 2024.
\newblock Ulmr: Unlearning large language models via negative response and model parameter average.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pages 755--762.

\bibitem[{Veldanda et~al.(2024)Veldanda, Zhang, Das, Chakraborty, Rawls, Sahu, and Naphade}]{veldanda2024llm}
Akshaj~Kumar Veldanda, Shi-Xiong Zhang, Anirban Das, Supriyo Chakraborty, Stephen Rawls, Sambit Sahu, and Milind Naphade. 2024.
\newblock Llm surgery: Efficient knowledge unlearning and editing in large language models.
\newblock \emph{arXiv preprint arXiv:2409.13054}.

\bibitem[{Wang et~al.(2024)Wang, Wei, Liu, Pang, Liu, Shah, Bao, Liu, and Wei}]{wang2024llm}
Yaxuan Wang, Jiaheng Wei, Chris~Yuhao Liu, Jinlong Pang, Quan Liu, Ankit~Parag Shah, Yujia Bao, Yang Liu, and Wei Wei. 2024.
\newblock Llm unlearning via loss adjustment with only forget data.
\newblock \emph{arXiv preprint arXiv:2410.11143}.

\end{thebibliography}
